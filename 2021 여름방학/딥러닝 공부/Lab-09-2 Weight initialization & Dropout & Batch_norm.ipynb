{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.manual_seed(777)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed(777)\n",
    "    \n",
    "# download\n",
    "mnist_train = dsets.MNIST(root = \"MNIST_data/\", train = True, transform=transforms.ToTensor(), download= True)\n",
    "mnist_test = dsets.MNIST(root = \"MNIST_data/\", train = False, transform=transforms.ToTensor(), download= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parmeter\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "training_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset=mnist_train,\n",
    "                          batch_size= batch_size,\n",
    "                          shuffle= True,\n",
    "                          drop_last= True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(dataset=mnist_test,\n",
    "                         batch_size=batch_size,\n",
    "                        shuffle=False,\n",
    "                        drop_last=False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0185, -0.0049, -0.0295,  ..., -0.0090, -0.0308, -0.0336],\n",
      "        [-0.0322,  0.0167,  0.0160,  ...,  0.0318,  0.0269, -0.0138],\n",
      "        [ 0.0090,  0.0111,  0.0108,  ...,  0.0267, -0.0091,  0.0140],\n",
      "        ...,\n",
      "        [ 0.0086, -0.0235, -0.0190,  ...,  0.0244, -0.0341,  0.0254],\n",
      "        [ 0.0162,  0.0298, -0.0347,  ...,  0.0017,  0.0320,  0.0342],\n",
      "        [-0.0012,  0.0350, -0.0184,  ...,  0.0091, -0.0125,  0.0024]])\n",
      "tensor([[ 6.8028e-02, -2.2149e-02, -1.7397e-02,  ...,  5.4531e-02,\n",
      "         -1.0376e-02,  9.1727e-03],\n",
      "        [ 3.0390e-02,  5.6982e-02,  6.9683e-02,  ..., -5.9019e-02,\n",
      "         -1.6356e-02, -6.0323e-02],\n",
      "        [-1.3146e-03, -1.2332e-02, -4.6129e-02,  ..., -6.4880e-05,\n",
      "          6.3648e-02,  3.6229e-02],\n",
      "        ...,\n",
      "        [ 1.6134e-02, -1.7151e-03, -1.9428e-02,  ...,  3.9911e-02,\n",
      "          4.2307e-02, -1.7196e-02],\n",
      "        [-7.6853e-03, -5.1876e-02,  2.9235e-02,  ...,  2.4711e-02,\n",
      "         -6.9202e-02,  5.6174e-02],\n",
      "        [ 1.3312e-02, -6.9033e-02, -4.9242e-02,  ...,  7.1279e-02,\n",
      "          1.4939e-03, -1.9624e-02]])\n"
     ]
    }
   ],
   "source": [
    "# 간단 wegiht initalization\n",
    "import torch.nn as nn\n",
    "\n",
    "net = nn.Linear(28*28,256)\n",
    "print(net.weight.data)\n",
    "\n",
    "nn.init.xavier_uniform_(net.weight)\n",
    "print(net.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 만들기\n",
    "# wegiht initalization 추가\n",
    "import torch.nn as nn\n",
    "\n",
    "class MnistClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(28*28, 256, bias= True),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            \n",
    "            nn.Linear(256,256, bias=True),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256,10,bias=True)\n",
    "            \n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.layer(x)\n",
    "      \n",
    "model = MnistClassifier().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight 초기화\n",
    "https://towardsdatascience.com/batch-normalization-and-dropout-in-neural-networks-explained-with-pytorch-47d7a8459bcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_uniform(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # apply a uniform distribution to the weights and a bias=0\n",
    "        #m.weight.data.uniform_(0.0, 1.0)\n",
    "        #m.bias.data.fill_(0)\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "# wegiht초기화는 처음만 시행하는 것 같아서 layer에서 따로 빼고, 함수를 만들어 주었다.        \n",
    "#model.apply(weights_init_uniform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost & optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iteration = len(train_loader)\n",
    "iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost : 0.3017\n",
      "Epoch: 0002 cost : 0.1123\n",
      "Epoch: 0003 cost : 0.0757\n",
      "Epoch: 0004 cost : 0.0547\n",
      "Epoch: 0005 cost : 0.0413\n",
      "Epoch: 0006 cost : 0.0316\n",
      "Epoch: 0007 cost : 0.0247\n",
      "Epoch: 0008 cost : 0.0232\n",
      "Epoch: 0009 cost : 0.0178\n",
      "Epoch: 0010 cost : 0.0158\n"
     ]
    }
   ],
   "source": [
    "iteration = len(train_loader)\n",
    "\n",
    "for epoch in range(training_epoch):\n",
    "    loss = 0\n",
    "    for batch_idx, sample in enumerate(train_loader):\n",
    "        X_train, Y_train = sample\n",
    "        X_train = X_train.view(-1, 28*28).to(device)\n",
    "        Y_train = Y_train.to(device)\n",
    "        \n",
    "        # test\n",
    "        hypothesis = model(X_train)\n",
    "        cost = criterion(hypothesis, Y_train)\n",
    "        \n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #calculate\n",
    "        loss += cost.item() / iteration\n",
    "        \n",
    "    print('Epoch: {:04d} cost : {:.4f}'.format(epoch+1, loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9736999869346619\n",
      "Label: tensor([8])\n",
      "pred 8\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "import random\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_test = mnist_test.test_data.view(-1,28*28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "    \n",
    "    #pred\n",
    "    pred = model(X_test)\n",
    "    correct_pred = torch.argmax(pred, 1) == Y_test\n",
    "    acc = correct_pred.float().mean()\n",
    "    print('Accuracy:', acc.item())\n",
    "    \n",
    "    # Get one\n",
    "    r = random.randint(0, len(mnist_test) -1)\n",
    "    X_single_data = mnist_test.test_data[r:r+1].view(-1,28*28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r+1].to(device)\n",
    "    \n",
    "    print(\"Label:\",Y_single_data)\n",
    "    single_pred = model(X_single_data)\n",
    "    print(\"pred\", torch.argmax(single_pred,1).item())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout 사용하기\n",
    "https://towardsdatascience.com/batch-normalization-and-dropout-in-neural-networks-explained-with-pytorch-47d7a8459bcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Dropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(28*28, 256),\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "            \n",
    "            \n",
    "            nn.Linear(256,256),\n",
    "#             nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            \n",
    "            nn.Linear(256,10),\n",
    "#             nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.layer(x)\n",
    "    \n",
    "model = Model_Dropout().to(device)\n",
    "#torch.nn.init.xavier_uniform_()\n",
    "weights_init_uniform(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost & optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost : 0.3055\n",
      "Epoch: 0002 cost : 0.1131\n",
      "Epoch: 0003 cost : 0.0714\n",
      "Epoch: 0004 cost : 0.0535\n",
      "Epoch: 0005 cost : 0.0406\n",
      "Epoch: 0006 cost : 0.0301\n",
      "Epoch: 0007 cost : 0.0235\n",
      "Epoch: 0008 cost : 0.0197\n",
      "Epoch: 0009 cost : 0.0188\n",
      "Epoch: 0010 cost : 0.0155\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(training_epoch):\n",
    "    loss = 0\n",
    "    for batch_idx, sample in enumerate(train_loader):\n",
    "        X_train, Y_train = sample\n",
    "        X_train = X_train.view(-1, 28*28).to(device)\n",
    "        Y_train = Y_train.to(device)\n",
    "        \n",
    "        # test\n",
    "        hypothesis = model(X_train)\n",
    "        cost = criterion(hypothesis, Y_train)\n",
    "        \n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #calculate\n",
    "        loss += cost.item() / iteration\n",
    "        \n",
    "    print('Epoch: {:04d} cost : {:.4f}'.format(epoch+1, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/igyuseog/opt/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:60: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/Users/igyuseog/opt/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9769999980926514\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "with torch.no_grad():\n",
    "    model.eval() # dropout 사용안함\n",
    "    \n",
    "    X_test = mnist_test.test_data.view(-1,28*28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "    \n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1).float() == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    \n",
    "    print(\"Accuracy:\",accuracy.item())\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch_Normalization\n",
    "- 사용이유: Internal Covatiate Shift(Layer마다 입력의 분포가 달라짐 --> 배치단위로 Normalize를 진행하였음)\n",
    "- 주의사항: Test와 같은 경우, Train에서 사용되는 Batchnorm의 평균,분산,감마,베타 값을 그대로 이용하므로 반드시 model.test() 써줘야함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Dropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            # layer1\n",
    "            nn.Linear(28*28, 256),\n",
    "            nn.BatchNorm1d(256), # n\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            #layer2\n",
    "            nn.Linear(256,10),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.layer(x)\n",
    "    \n",
    "model = Model_Dropout().to(device)\n",
    "#torch.nn.init.xavier_uniform_()\n",
    "weights_init_uniform(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost & optim\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Cost: 0.3294\n",
      "Acc : 0.9571\n",
      "Epoch: 0002 Cost: 0.1165\n",
      "Acc : 0.9705\n",
      "Epoch: 0003 Cost: 0.0717\n",
      "Acc : 0.9776\n",
      "Epoch: 0004 Cost: 0.0495\n",
      "Acc : 0.9762\n",
      "Epoch: 0005 Cost: 0.0363\n",
      "Acc : 0.9741\n",
      "Epoch: 0006 Cost: 0.0270\n",
      "Acc : 0.9794\n",
      "Epoch: 0007 Cost: 0.0214\n",
      "Acc : 0.9785\n",
      "Epoch: 0008 Cost: 0.0174\n",
      "Acc : 0.9794\n",
      "Epoch: 0009 Cost: 0.0141\n",
      "Acc : 0.9786\n",
      "Epoch: 0010 Cost: 0.0130\n",
      "Acc : 0.9803\n"
     ]
    }
   ],
   "source": [
    "model.train() #dropout, batchnorm이 원활히 적용.\n",
    "\n",
    "iteration = len(train_loader)\n",
    "for epoch in range(training_epoch):\n",
    "    loss = 0\n",
    "    for batch_idx, sample in enumerate(train_loader):\n",
    "        X,Y = sample\n",
    "        X_train = X.view(-1,28*28).float().to(device)\n",
    "        Y_train = Y.to(device)\n",
    "        \n",
    "        hypothesis = model(X_train)\n",
    "        cost = criterion(hypothesis, Y_train)\n",
    "        \n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #cal\n",
    "        loss += cost.item() /iteration\n",
    "        \n",
    "    print(\"Epoch: {:04d} Cost: {:.4f}\".format(epoch+1, loss))\n",
    "    \n",
    "    #test\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        acc = 0\n",
    "        for idx, (X,Y) in enumerate(test_loader):\n",
    "            X = X.view(-1,28*28).float().to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            correct_pred = torch.argmax(pred,1) == Y\n",
    "            acc += correct_pred.float().mean()\n",
    "            \n",
    "        print(\"Acc : {:.4f}\".format(acc/len(test_loader)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
