{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제곱 합 오차 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def MSEcost(self, A2, Y):\n",
    "    E2 = Y - A2\n",
    "    cost = np.sqrt(np.sum(E2 * E2))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 교차 엔트로피 손실 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CEcost(A2, Y):\n",
    "    m = Y.shape[0] # number of example\n",
    "    logprobs = np.multiply(Y, np.log(A2))\n",
    "    cost = -np.sum(logprobs) / m\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.30258509 0.         0.        ]\n",
      "0.7675283643313485\n"
     ]
    }
   ],
   "source": [
    "ans = np.array([1,0,0])\n",
    "yhat = np.array([0.1,0.8,0.3])\n",
    "logprobs = -np.multiply(ans, np.log(yhat))\n",
    "print(logprobs)\n",
    "cost = np.sum(logprobs)/3\n",
    "cost = np.squeeze(cost)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 소프트 맥스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(self, a):\n",
    "    exp_a = np.exp(a - np.max(a))\n",
    "    return exp_a / np.sum(exp_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 로지스틱 회귀 구현: 교차 엔트로피를 이용한 경사하강법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile code/LogisticNeuron_stochastic.py\n",
    "#%load code/LogisticNeuron_stochastic.py\n",
    "# author: idebtor@gmail.com\n",
    "# import external libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "class LogisticNeuron_stochastic(object):\n",
    "    \"\"\"implements Logistic Regression using cross entropy with stochastic gradient descent\"\"\"\n",
    "    def __init__(self, n_x, n_h, n_y, eta = 0.2, epochs = 5, random_seed=1):\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "        np.random.seed(self.random_seed)\n",
    "        self.W1 = 2*np.random.random((self.n_h, self.n_x)) - 1\n",
    "        self.b1 = np.zeros((self.n_h, 1))\n",
    "        self.W2 = 2*np.random.random((self.n_y, self.n_h)) - 1      \n",
    "        self.b2 = np.zeros((self.n_y, 1))\n",
    "        self.W1 = 2*np.random.random((self.n_h, self.n_x)) - 1  \n",
    "        self.W2 = 2*np.random.random((self.n_y, self.n_h)) - 1  \n",
    "        \n",
    "    def CEcost(self, A2, Y):\n",
    "        m = Y.shape[1]      # number of example\n",
    "        logprobs = np.multiply(Y, np.log(A2))\n",
    "        cost = -np.sum(logprobs)/m\n",
    "        cost = np.squeeze(cost)        \n",
    "        return cost  \n",
    "    \n",
    "    def forpass(self, A0):\n",
    "        Z1 = np.dot(self.W1, A0) + self.b1         \n",
    "        A1 = self.g(Z1)    #은닉층에서는 시그모이드를 사용                      \n",
    "        Z2 = np.dot(self.W2, A1) + self.b2       \n",
    "        A2 = self.softmax(Z2)   #출력층에서는 softmax 함수를 사용                \n",
    "        return Z1, A1, Z2, A2\n",
    "\n",
    "    def fit(self, X, y): \n",
    "        self.cost_ = []\n",
    "        self.m_samples = len(y)\n",
    "        Y = joy.one_hot_encoding(y, self.n_y)       # (m, n_y) = (m, 10)   one-hot encoding\n",
    "               \n",
    "        for epoch in range(self.epochs):           \n",
    "            for sample in range(self.m_samples):            \n",
    "                A0 = np.array(X[sample], ndmin=2).T  \n",
    "                Y0 = np.array(Y[sample], ndmin=2).T  \n",
    "\n",
    "                Z1, A1, Z2, A2 = self.forpass(A0)          # forward pass\n",
    "                \n",
    "                # Cost function: Compute the cross-entropy cost\n",
    "                cost = self.CEcost(A2, Y0)\n",
    "                self.cost_.append(cost)\n",
    "                # Backpropagation. \n",
    "                E2 = Y0 - A2                \n",
    "                dZ2 = E2 \n",
    "                dW2 = np.dot(dZ2, A1.T) / self.m_samples\n",
    "                db2 = np.sum(dZ2, axis=1, keepdims=True) / self.m_samples\n",
    "                \n",
    "                E1 = np.dot(self.W2.T, E2)  \n",
    "                dZ1 = E1 * self.g_prime(Z1)  #sigmoid\n",
    "                #dZ1 = E1 * (1 - np.power(A1, 2)) #tanh\n",
    "                dW1 = np.dot(dZ1, A0.T) \n",
    "                db1 = np.sum(dZ1, axis=1, keepdims=True) \n",
    "                \n",
    "                # update weights \n",
    "                self.W1 += self.eta * dW1 \n",
    "                self.b1 += self.eta * db1 \n",
    "                self.W2 += self.eta * dW2 \n",
    "                self.b2 += self.eta * db2 \n",
    "            print('Training epoch {}/{}, cost = {}'.format(epoch+1, self.epochs, cost))\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        A0 = np.array(X, ndmin=2).T         # A0: inputs\n",
    "        Z1, A1, Z2, A2 = self.forpass(A0)   # forpass\n",
    "        return A2  \n",
    "\n",
    "    def g(self, x):                 # activation_function: sigmoid\n",
    "        x = np.clip(x, -500, 500)   # prevent from overflow, \n",
    "        return 1.0/(1.0+np.exp(-x)) # stackoverflow.com/questions/23128401/\n",
    "                                    # overflow-error-in-neural-networks-implementation\n",
    "    \n",
    "    def g_prime(self, x):           # activation_function: sigmoid derivative\n",
    "        return self.g(x) * (1 - self.g(x))\n",
    "    \n",
    "    def softmax(self, a):           # prevent it from overlfow and undeflow\n",
    "        exp_a = np.exp(a - np.max(a))\n",
    "        return exp_a / np.sum(exp_a)\n",
    "    \n",
    "    def evaluate(self, Xtest, ytest):   # fully vectorized calculation\n",
    "        m_samples = len(ytest)  \n",
    "        A2 = self.predict(Xtest)\n",
    "        yhat = np.argmax(A2, axis = 0)\n",
    "        scores = np.sum(yhat == ytest)\n",
    "        return scores/m_samples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joy\n",
    "(X, y), (Xtest, ytest) = joy.load_mnist()\n",
    "nn = LogisticNeuron_stochastic(784, 100, 10, eta = 0.2, epochs = 3)  \n",
    "nn.fit(X, y)       \n",
    "self_accuracy = nn.evaluate(X, y)\n",
    "test_accuracy = nn.evaluate(Xtest, ytest)\n",
    "print('self_accuracy=', self_accuracy)  # 96.35\n",
    "print('test_accuracy=', test_accuracy)  # 95.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joy\n",
    "(X, y), (Xtest, ytest) = joy.load_mnist()\n",
    "self_accuracy = []\n",
    "test_accuracy = []\n",
    "epoch_list = np.arange(1, 5)\n",
    "for e in epoch_list:\n",
    "    nn = joy.LogisticNeuron_stochastic(784, 100, 10, eta = 0.2, epochs = e)  \n",
    "    nn.fit(X, y)       \n",
    "    self_accuracy.append(nn.evaluate(X, y))  \n",
    "    test_accuracy.append(nn.evaluate(Xtest, ytest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
