{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1307]) tensor([0.3080])\n",
      "cpu\n",
      "Weight init Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/igyuseog/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:118: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0010], Loss = 0.0067, acc = 9.83\n",
      "[Epoch 0020], Loss = 0.0051, acc = 10.14\n",
      "[Test] Loss = 0.0442, acc = 0.99\n",
      "[Epoch 0030], Loss = 0.0032, acc = 9.90\n",
      "[Epoch 0040], Loss = 0.0033, acc = 10.00\n",
      "[Test] Loss = 0.0506, acc = 0.99\n",
      "[Epoch 0050], Loss = 0.0000, acc = 9.95\n",
      "[Epoch 0060], Loss = 0.0000, acc = 10.15\n",
      "[Test] Loss = 0.0660, acc = 0.99\n",
      "[Epoch 0070], Loss = 0.0000, acc = 9.88\n",
      "[Epoch 0080], Loss = 0.0000, acc = 10.05\n",
      "[Test] Loss = 0.0830, acc = 0.99\n",
      "[Epoch 0090], Loss = 0.0000, acc = 9.96\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dsets # 데이터셋\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader # customData, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "mnist_train = dsets.MNIST(root = \"MNIST_data\", train = True, download= True, transform = transforms.ToTensor())\n",
    "mnist_test = dsets.MNIST(root = \"MNIST_data\", train = False, download= True, transform = transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(mnist_train, batch_size = 100, shuffle= True, drop_last = True)\n",
    "test_loader = DataLoader(mnist_test, batch_size = 100, shuffle = False, drop_last = False)\n",
    "\n",
    "\n",
    "\n",
    "# CustomDataset\n",
    "#(여기서는 쓸게 아니니까 괜찮음)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y, transform = None):\n",
    "        self.x_data = X\n",
    "        self.y_data = Y\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem(self,idx):\n",
    "        \n",
    "        x = self.x_data[idx]\n",
    "        y = self.y_data[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "            y = self.transform(y)\n",
    "        \n",
    "        return x,y\n",
    "\n",
    "# 정규화 함수\n",
    "\n",
    "def get_mean_std(loader):\n",
    "    mu = 0\n",
    "    std = 0\n",
    "    for sample in loader:\n",
    "        X,Y = sample       \n",
    "        mu += torch.mean(X, dim = [0,2,3])\n",
    "        std += torch.std(X, dim = [0,2,3])\n",
    "        \n",
    "    mu /= len(loader)\n",
    "    std /= len(loader)\n",
    "    return mu, std\n",
    "\n",
    "\n",
    "mu, std = get_mean_std(train_loader)\n",
    "print(mu,std)\n",
    "\n",
    "mu.item()\n",
    "\n",
    "# Normalize \n",
    "\n",
    "trans = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((mu.item()),(std.item()))\n",
    "])\n",
    "\n",
    "train_mnist = dsets.MNIST(root = \"MNIST_data/\", download= True, transform= trans, train = True)\n",
    "test_mnist = dsets.MNIST(root = \"MNIST_data/\", download= True, transform= trans, train = False)\n",
    "\n",
    "train_loader = DataLoader(train_mnist, batch_size = 100, shuffle= True, drop_last= True)\n",
    "test_loader = DataLoader(test_mnist, batch_size = 100, shuffle= False, drop_last= False)\n",
    "\n",
    "\n",
    "get_mean_std(train_loader)\n",
    "\n",
    "# step2) device\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(777)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "print(device)\n",
    "    \n",
    "# step3) hyper-parameter\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "training_epoch = 100\n",
    "\n",
    "# step4) Dataset & DataLoader --> 위에서 이미 구현\n",
    "\n",
    "# step5) 모델링\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN,self).__init__()\n",
    "        \n",
    "        # 1*28*28\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 128, kernel_size= 3, padding= 1, stride = 2),\n",
    "            # 128*14*14\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        \n",
    "        # 128*7*7 \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding = 1, stride = 2),\n",
    "            # 256*3*3\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # 256*3*3\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256,512, kernel_size= 3),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # 512 * 1 *1\n",
    "        \n",
    "        def forward(self,x):\n",
    "            out = self.layer1(x)\n",
    "            print(out.shape)\n",
    "            \n",
    "            out = self.layer2(out)\n",
    "            print(out.shape)\n",
    "            \n",
    "            out = self.layer3(out)\n",
    "            print(out.shape)\n",
    "            \n",
    "            return out\n",
    "\n",
    "model = CNN(10).to(device)\n",
    "model\n",
    "\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super(CNN,self).__init__()\n",
    "        \n",
    "         # 1*28*28\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 128, kernel_size= 3, padding= 1, stride = 2),\n",
    "            # 128*14*14\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        \n",
    "        # 128*7*7 \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding = 1, stride = 2),\n",
    "            # 256*4*4\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # 256*3*3\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256,512, kernel_size= 3),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "            out = self.layer1(x)            \n",
    "            out = self.layer2(out)            \n",
    "            out = self.layer3(out)            \n",
    "            out = out.view(out.size(0), -1)\n",
    "            out = self.fc(out)            \n",
    "            return out\n",
    "        \n",
    "    def initalize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode = \"fan_out\", nonlinearity= \"relu\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight,1)\n",
    "                nn.init.constant_(m.bias,0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight,1)\n",
    "                nn.init.constant_(m.bias,0)\n",
    "        print(\"Weight init Done\")\n",
    "    \n",
    "model = CNN(10).to(device)\n",
    "model.initalize_weights()\n",
    "\n",
    "\n",
    "# step6) loss & optim\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "lr_sche = optim.lr_scheduler.StepLR(optimizer, 100, gamma= 0.9)\n",
    "\n",
    "\n",
    "# step7,8) train & test\n",
    "iteration = len(train_loader)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(training_epoch):\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    lr_sche.step()\n",
    "    for sample in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        X_train, Y_train = sample\n",
    "        X_train = X_train.to(device)\n",
    "        Y_train = Y_train.to(device)\n",
    "        \n",
    "        hypothesis = model(X_train)\n",
    "        cost = criterion(hypothesis, Y_train)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #calculate\n",
    "        loss += cost.item()\n",
    "        correct += (torch.argmax(hypothesis, dim = 1) == Y_train).float().sum()\n",
    "        \n",
    "    loss /= iteration\n",
    "    acc = correct / (batch_size*iteration)\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(\"[Epoch {:04d}], Loss = {:.4f}, acc = {:.2f}\".format(epoch+1, loss, acc*100))\n",
    "        \n",
    "    if (epoch+1) %20 == 0:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            t_loss = 0\n",
    "            t_correct = 0\n",
    "            for X_test, Y_test in test_loader:\n",
    "                X_test = X_test.to(device)\n",
    "                Y_test = Y_test.to(device)\n",
    "                \n",
    "                pred = model(X_test)\n",
    "                t_cost = criterion(pred, Y_test)\n",
    "                \n",
    "                #calculate\n",
    "                t_loss += t_cost.item()\n",
    "                t_correct += (torch.argmax(pred, dim = 1) == Y_test).float().sum()\n",
    "                \n",
    "            t_loss /= len(test_loader)\n",
    "            t_acc = t_correct / (len(test_loader)*batch_size)\n",
    "            print(\"[Test] Loss = {:.4f}, acc = {:.2f}\".format(t_loss, t_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
