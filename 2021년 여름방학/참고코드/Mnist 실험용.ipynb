{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1) 모듈 불러오기\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# step2) device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.manual_seed(777)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stpe3) hyper parameter\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "training_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step4) Dataset(transform) & Dataloader\n",
    "\n",
    "trans = transforms.Compose([\n",
    "#     transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307), (0.3081))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# 4-1) dataset 가져오기\n",
    "mnist_train = dsets.MNIST(root = \"../data/MNIST_data/\", train= True, transform = trans, download= True)\n",
    "mnist_test = dsets.MNIST(root = \"../data/MNIST_data/\", train= False, transform = trans, download= True)\n",
    "\n",
    "# 4-2) loader 만들기\n",
    "train_loader = DataLoader(dataset= mnist_train, batch_size= batch_size, shuffle= True, drop_last= True)\n",
    "test_loader = DataLoader(dataset= mnist_test, batch_size= batch_size, shuffle= False, drop_last= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0001]), tensor([1.0000]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_mean_std(loader):\n",
    "    mu = 0\n",
    "    mu_square = 0\n",
    "    std = 0\n",
    "    for sample in loader:\n",
    "        X,Y = sample\n",
    "\n",
    "        mu += torch.mean(X, dim = [0,2,3])\n",
    "        mu_square += torch.mean(X**2, dim = [0,2,3])\n",
    "        std += torch.std(X, dim = [0,2,3])\n",
    "\n",
    "    mu /= len(loader)\n",
    "    mu_square /= len(loader)\n",
    "    std = (mu_square - mu**2)**0.5 # E[X**2] - {E[X]}**2\n",
    "    return mu, std\n",
    "\n",
    "get_mean_std(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "here1\n",
      "here2\n",
      "here1\n",
      "here2\n",
      "here3\n",
      "here4\n",
      "here3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step5) 모델링\n",
    "# 간단한 conv + depthwise separable conv + fc1 + fc2\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        # 1*28*28\n",
    "        self.conv_layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1,128,3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        # 128*14*14\n",
    "        self.depthwise = nn.Sequential(\n",
    "            nn.Conv2d(128, 128*3, 3, groups= 128, padding=1),\n",
    "            nn.BatchNorm2d(128*3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        # 384*7*7\n",
    "        self.pointwise = nn.Sequential(\n",
    "            nn.Conv2d(384,512, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "        )\n",
    "        \n",
    "        # 512*3*3\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(512*3*3, 100),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2), # dropout 추가\n",
    "            \n",
    "        )\n",
    "        \n",
    "        # 100\n",
    "        self.fc2 = nn.Linear(100,10)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.conv_layer1(x)\n",
    "        \n",
    "        out = self.depthwise(out)\n",
    "        \n",
    "        out = self.pointwise(out)\n",
    "        \n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def weight_initalzie(self):\n",
    "        for m in self.modules():\n",
    "            \n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                print(\"here1\")\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            if isinstance(m,nn.BatchNorm2d):\n",
    "                print(\"here2\")\n",
    "\n",
    "                nn.init.constant_(m.weight,1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            if isinstance(m,nn.Linear):\n",
    "                print(\"here3\")\n",
    "\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            if isinstance(m, nn.BatchNorm1d):\n",
    "                print(\"here4\")\n",
    "\n",
    "                nn.init.constant_(m.weight,1)\n",
    "                nn.init.constant_(m.weight,0)\n",
    "                \n",
    "                \n",
    "    \n",
    "model = MyModel().to(device)\n",
    "\n",
    "# init\n",
    "model.weight_initalzie()\n",
    "\n",
    "test_data = torch.Tensor(2,1,28,28)\n",
    "model(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step6) loss & optim & lr_scheduler\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "lr_sche = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0001], loss = 2.3016, acc = 11.10, lr = 0.0010\n",
      "[Epoch 0002], loss = 2.3013, acc = 11.24, lr = 0.0010\n"
     ]
    }
   ],
   "source": [
    "# step7) train\n",
    "\n",
    "iteration = len(train_loader)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    \n",
    "    lr_sche.step()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for X,Y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        \n",
    "        # forward, backward, optim\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #calculate\n",
    "        loss += cost.item()\n",
    "        correct += (torch.argmax(hypothesis, dim = 1) == Y).float().sum()\n",
    "    \n",
    "    loss /= iteration\n",
    "    acc = correct / (batch_size * iteration)\n",
    "    \n",
    "    print(\"[Epoch {:04d}], loss = {:.4f}, acc = {:.2f}, lr = {:.4f}\".format(epoch+1, loss, acc*100, learning_rate))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step8) test\n",
    "\n",
    "test_iteration = len(test_loader)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for sample in test_loader:\n",
    "        X_test, Y_test = sample\n",
    "        X_test = X_test.to(device)\n",
    "        Y_test = Y_test.to(device)\n",
    "        \n",
    "        # test\n",
    "        hypothesis = model(X_test)\n",
    "        cost = criterion(hypothesis,Y_test)\n",
    "        \n",
    "        # calculate\n",
    "        loss += cost.item()\n",
    "        correct += (torch.argmax(hypothesis, 1) == Y_test).float().sum()\n",
    "        \n",
    "    loss /= test_iteration\n",
    "    acc = correct / (batch_size * test_iteration)\n",
    "    \n",
    "    print(\"[Test] loss = {:.4f}, Acc = {:.2f}\".format(loss,acc*100))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
