{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 퍼셉트론\n",
    "\n",
    "- 신경망을 본떠서 만들어진 neural net이 OR, AND문제를 해결할 수 있을 것이라고 여겨짐.\n",
    "- 그러나 XOR문제를 Linear하게 해결할 수 있는 방법이 존재하지 않으며, Multi-layer의 필요성이 대두됌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# for reproducibility\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = torch.FloatTensor([[0],[1],[1],[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "linear = torch.nn.Linear(2,1, bias = True)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "model = torch.nn.Sequential(linear, sigmoid).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 cost : 0.7274, acc : 50.000000\n",
      "step: 1000 cost : 0.6931, acc : 50.000000\n",
      "step: 2000 cost : 0.6931, acc : 50.000000\n",
      "step: 3000 cost : 0.6931, acc : 50.000000\n",
      "step: 4000 cost : 0.6931, acc : 50.000000\n",
      "step: 5000 cost : 0.6931, acc : 50.000000\n",
      "step: 6000 cost : 0.6931, acc : 50.000000\n",
      "step: 7000 cost : 0.6931, acc : 50.000000\n",
      "step: 8000 cost : 0.6931, acc : 50.000000\n",
      "step: 9000 cost : 0.6931, acc : 50.000000\n",
      "step: 10000 cost : 0.6931, acc : 50.000000\n"
     ]
    }
   ],
   "source": [
    "for step in range(10001):\n",
    "    hypothesis = model(X)\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    if step% 1000 == 0:\n",
    "        acc = ((hypothesis > torch.FloatTensor([0.5])).float() == Y).sum().item()/ len(Y)\n",
    "        print('step: {} cost : {:.4f}, acc : {:4f}'.format(step, cost.item(), acc*100))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi layer perceptron\n",
    "\n",
    "- Linear한 형태로 XOR문제를 해결할 수 없었음 (Single layer perceptron의 한계)\n",
    "- Layer가 여러 개인 경우에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor([[0,0],[0,1],[1,0],[1,1]]).to(device)\n",
    "Y = torch.FloatTensor([[0],[1],[1],[0]]).to(device)\n",
    "\n",
    "w1 = torch.Tensor(2,2).to(device)\n",
    "b1 = torch.Tensor(2).to(device)\n",
    "\n",
    "w2 = torch.Tensor(2,1).to(device)\n",
    "b2 = torch.Tensor(1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + torch.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "for step in range(10001):\n",
    "    ## forward\n",
    "    l1 = torch.add(torch.matmul(X,w1),b1)\n",
    "    a1 = sigmoid(l1)\n",
    "    l2 = torch.add(torch.matmul(a1,w2),b2)\n",
    "    Y_pred = sigmoid(l2)\n",
    "    \n",
    "    # BCE \n",
    "    cost = -torch.mean(Y*torch.log(Y_pred) + (1-Y)*torch.log(1-Y_pred))\n",
    "    \n",
    "    ## backward\n",
    "    d_Y_pred = (Y_pred - Y) / (Y_pred * (1.0 - Y_pred) + 1e-7)  b\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code: Xor-nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "# for reproducibility\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor([[0,0],[0,1],[1,0],[1,1]]).to(device)\n",
    "Y = torch.FloatTensor([[0],[1],[1],[0]]).to(device)\n",
    "\n",
    "# nn layers\n",
    "linear1 = torch.nn.Linear(2,10,bias = True)\n",
    "linear2 = torch.nn.Linear(10,10,bias = True)\n",
    "linear3 = torch.nn.Linear(10,10,bias = True)\n",
    "linear4 = torch.nn.Linear(10,1,bias = True)\n",
    "\n",
    "\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid, linear3, sigmoid, linear4, sigmoid).to(device)\n",
    "\n",
    "#define cost/Loss & Optimizer\n",
    "criterion = torch.nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7050673961639404 50.0\n",
      "100 0.6931442022323608 50.0\n",
      "200 0.6931438446044922 50.0\n",
      "300 0.6931435465812683 50.0\n",
      "400 0.6931431889533997 50.0\n",
      "500 0.6931427717208862 50.0\n",
      "600 0.6931424140930176 50.0\n",
      "700 0.6931420564651489 50.0\n",
      "800 0.6931416988372803 50.0\n",
      "900 0.6931412816047668 50.0\n",
      "1000 0.6931408643722534 50.0\n",
      "1100 0.6931403875350952 50.0\n",
      "1200 0.693139910697937 50.0\n",
      "1300 0.6931394338607788 50.0\n",
      "1400 0.6931389570236206 50.0\n",
      "1500 0.6931384801864624 50.0\n",
      "1600 0.6931380033493042 50.0\n",
      "1700 0.6931374073028564 50.0\n",
      "1800 0.6931368112564087 50.0\n",
      "1900 0.6931362152099609 50.0\n",
      "2000 0.6931355595588684 50.0\n",
      "2100 0.6931349039077759 50.0\n",
      "2200 0.6931341886520386 50.0\n",
      "2300 0.6931334733963013 50.0\n",
      "2400 0.693132758140564 50.0\n",
      "2500 0.6931319236755371 50.0\n",
      "2600 0.6931309700012207 50.0\n",
      "2700 0.6931300163269043 50.0\n",
      "2800 0.6931290626525879 50.0\n",
      "2900 0.6931280493736267 50.0\n",
      "3000 0.6931268572807312 50.0\n",
      "3100 0.6931256651878357 50.0\n",
      "3200 0.6931244134902954 50.0\n",
      "3300 0.6931229829788208 50.0\n",
      "3400 0.6931214332580566 50.0\n",
      "3500 0.6931198835372925 50.0\n",
      "3600 0.693118155002594 50.0\n",
      "3700 0.6931162476539612 50.0\n",
      "3800 0.693114161491394 50.0\n",
      "3900 0.6931118965148926 75.0\n",
      "4000 0.693109393119812 75.0\n",
      "4100 0.6931067705154419 75.0\n",
      "4200 0.6931036710739136 75.0\n",
      "4300 0.6931003928184509 75.0\n",
      "4400 0.6930967569351196 75.0\n",
      "4500 0.6930925846099854 75.0\n",
      "4600 0.6930878758430481 75.0\n",
      "4700 0.6930826306343079 75.0\n",
      "4800 0.6930767297744751 50.0\n",
      "4900 0.6930699348449707 50.0\n",
      "5000 0.6930621862411499 50.0\n",
      "5100 0.6930530071258545 50.0\n",
      "5200 0.6930423378944397 50.0\n",
      "5300 0.6930298209190369 50.0\n",
      "5400 0.6930148005485535 50.0\n",
      "5500 0.692996621131897 50.0\n",
      "5600 0.692974328994751 50.0\n",
      "5700 0.6929464340209961 50.0\n",
      "5800 0.69291090965271 50.0\n",
      "5900 0.6928646564483643 50.0\n",
      "6000 0.6928026676177979 50.0\n",
      "6100 0.6927165389060974 50.0\n",
      "6200 0.6925919651985168 50.0\n",
      "6300 0.6924008727073669 50.0\n",
      "6400 0.6920854449272156 50.0\n",
      "6500 0.6915065050125122 50.0\n",
      "6600 0.6902614831924438 50.0\n",
      "6700 0.6867905259132385 75.0\n",
      "6800 0.6714922189712524 75.0\n",
      "6900 0.5791335105895996 75.0\n",
      "7000 0.30054718255996704 100.0\n",
      "7100 0.013067862950265408 100.0\n",
      "7200 0.005605909042060375 100.0\n",
      "7300 0.0034131030552089214 100.0\n",
      "7400 0.0024080381263047457 100.0\n",
      "7500 0.0018417984247207642 100.0\n",
      "7600 0.001482105813920498 100.0\n",
      "7700 0.0012348988093435764 100.0\n",
      "7800 0.0010553407482802868 100.0\n",
      "7900 0.000919381040148437 100.0\n",
      "8000 0.0008131322101689875 100.0\n",
      "8100 0.0007279803394339979 100.0\n",
      "8200 0.0006582835922017694 100.0\n",
      "8300 0.0006002513691782951 100.0\n",
      "8400 0.0005512129864655435 100.0\n",
      "8500 0.0005092885112389922 100.0\n",
      "8600 0.000473075604531914 100.0\n",
      "8700 0.00044144075945951045 100.0\n",
      "8800 0.00041362320189364254 100.0\n",
      "8900 0.00038896669866517186 100.0\n",
      "9000 0.0003669791331049055 100.0\n",
      "9100 0.00034725776640698314 100.0\n",
      "9200 0.0003294448251836002 100.0\n",
      "9300 0.00031334636150859296 100.0\n",
      "9400 0.00029866420663893223 100.0\n",
      "9500 0.00028524917433969676 100.0\n",
      "9600 0.0002729671832639724 100.0\n",
      "9700 0.0002616392448544502 100.0\n",
      "9800 0.0002511163183953613 100.0\n",
      "9900 0.0002414728660369292 100.0\n",
      "10000 0.00023248531215358526 100.0\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "for step in range(10001):\n",
    "    hypothesis = model(X)\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    # back\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # calculate\n",
    "    acc =(((hypothesis > torch.FloatTensor([0.5])).float() == Y).sum().item() / len(Y)) *100\n",
    "\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(step, cost.item(), acc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
