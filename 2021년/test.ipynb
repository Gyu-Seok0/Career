{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_edge = tensor([[-1,-1,-1],\n",
    "                  [0,0,0],\n",
    "                  [1,1,1]]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_basics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-38ee53a4a02f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_basics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_basics'"
     ]
    }
   ],
   "source": [
    "# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/04_data.external.ipynb (unless otherwise specified).\n",
    "\n",
    "__all__ = ['Config', 'URLs', 'download_url', 'download_data', 'file_extract', 'newest_folder', 'rename_extracted',\n",
    "           'untar_data']\n",
    "\n",
    "# Cell\n",
    "from torch_basics import *\n",
    "\n",
    "# Cell\n",
    "class Config:\n",
    "    \"Setup config at `~/.fastai` unless it exists already.\"\n",
    "    config_path = Path(os.getenv('FASTAI_HOME', '~/.fastai')).expanduser()\n",
    "    config_file = config_path/'config.yml'\n",
    "\n",
    "    def __init__(self):\n",
    "        self.config_path.mkdir(parents=True, exist_ok=True)\n",
    "        if not self.config_file.exists(): self.create_config()\n",
    "        self.d = self.load_config()\n",
    "\n",
    "    def __getitem__(self,k):\n",
    "        k = k.lower()\n",
    "        if k not in self.d: k = k+'_path'\n",
    "        return Path(self.d[k])\n",
    "\n",
    "    def __getattr__(self,k):\n",
    "        if k=='d': raise AttributeError\n",
    "        return self[k]\n",
    "\n",
    "    def __setitem__(self,k,v): self.d[k] = str(v)\n",
    "    def __contains__(self,k): return k in self.d\n",
    "\n",
    "    def load_config(self):\n",
    "        \"load and return config if version equals 2 in existing, else create new config.\"\n",
    "        with open(self.config_file, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "            if 'version' in config and config['version'] == 2: return config\n",
    "            elif 'version' in config: self.create_config(config)\n",
    "            else: self.create_config()\n",
    "        return self.load_config()\n",
    "\n",
    "    def create_config(self, cfg=None):\n",
    "        \"create new config with default paths and set `version` to 2.\"\n",
    "        config = {'data_path':    str(self.config_path/'data'),\n",
    "                  'archive_path': str(self.config_path/'archive'),\n",
    "                  'storage_path': '/tmp',\n",
    "                  'model_path':   str(self.config_path/'models'),\n",
    "                  'version':      2}\n",
    "        if cfg is not None:\n",
    "            cfg['version'] = 2\n",
    "            config = merge(config, cfg)\n",
    "        self.save_file(config)\n",
    "\n",
    "    def save(self): self.save_file(self.d)\n",
    "    def save_file(self, config):\n",
    "        \"save config file at default config location `~/.fastai/config.yml`.\"\n",
    "        with self.config_file.open('w') as f: yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "# Cell\n",
    "class URLs():\n",
    "    \"Global constants for dataset and model URLs.\"\n",
    "    LOCAL_PATH = Path.cwd()\n",
    "    MDL = 'http://files.fast.ai/models/'\n",
    "    S3  = 'https://s3.amazonaws.com/fast-ai-'\n",
    "    URL = f'{S3}sample/'\n",
    "\n",
    "    S3_IMAGE    = f'{S3}imageclas/'\n",
    "    S3_IMAGELOC = f'{S3}imagelocal/'\n",
    "    S3_AUDI     = f'{S3}audio/'\n",
    "    S3_NLP      = f'{S3}nlp/'\n",
    "    S3_COCO     = f'{S3}coco/'\n",
    "    S3_MODEL    = f'{S3}modelzoo/'\n",
    "\n",
    "    # main datasets\n",
    "    ADULT_SAMPLE        = f'{URL}adult_sample.tgz'\n",
    "    BIWI_SAMPLE         = f'{URL}biwi_sample.tgz'\n",
    "    CIFAR               = f'{URL}cifar10.tgz'\n",
    "    COCO_SAMPLE         = f'{S3_COCO}coco_sample.tgz'\n",
    "    COCO_TINY           = f'{S3_COCO}coco_tiny.tgz'\n",
    "    HUMAN_NUMBERS       = f'{URL}human_numbers.tgz'\n",
    "    IMDB                = f'{S3_NLP}imdb.tgz'\n",
    "    IMDB_SAMPLE         = f'{URL}imdb_sample.tgz'\n",
    "    ML_SAMPLE           = f'{URL}movie_lens_sample.tgz'\n",
    "    ML_100k             = 'https://files.grouplens.org/datasets/movielens/ml-100k.zip'\n",
    "    MNIST_SAMPLE        = f'{URL}mnist_sample.tgz'\n",
    "    MNIST_TINY          = f'{URL}mnist_tiny.tgz'\n",
    "    MNIST_VAR_SIZE_TINY = f'{S3_IMAGE}mnist_var_size_tiny.tgz'\n",
    "    PLANET_SAMPLE       = f'{URL}planet_sample.tgz'\n",
    "    PLANET_TINY         = f'{URL}planet_tiny.tgz'\n",
    "    IMAGENETTE          = f'{S3_IMAGE}imagenette2.tgz'\n",
    "    IMAGENETTE_160      = f'{S3_IMAGE}imagenette2-160.tgz'\n",
    "    IMAGENETTE_320      = f'{S3_IMAGE}imagenette2-320.tgz'\n",
    "    IMAGEWOOF           = f'{S3_IMAGE}imagewoof2.tgz'\n",
    "    IMAGEWOOF_160       = f'{S3_IMAGE}imagewoof2-160.tgz'\n",
    "    IMAGEWOOF_320       = f'{S3_IMAGE}imagewoof2-320.tgz'\n",
    "    IMAGEWANG           = f'{S3_IMAGE}imagewang.tgz'\n",
    "    IMAGEWANG_160       = f'{S3_IMAGE}imagewang-160.tgz'\n",
    "    IMAGEWANG_320       = f'{S3_IMAGE}imagewang-320.tgz'\n",
    "\n",
    "    # kaggle competitions download dogs-vs-cats -p {DOGS.absolute()}\n",
    "    DOGS = f'{URL}dogscats.tgz'\n",
    "\n",
    "    # image classification datasets\n",
    "    CALTECH_101  = f'{S3_IMAGE}caltech_101.tgz'\n",
    "    CARS         = f'{S3_IMAGE}stanford-cars.tgz'\n",
    "    CIFAR_100    = f'{S3_IMAGE}cifar100.tgz'\n",
    "    CUB_200_2011 = f'{S3_IMAGE}CUB_200_2011.tgz'\n",
    "    FLOWERS      = f'{S3_IMAGE}oxford-102-flowers.tgz'\n",
    "    FOOD         = f'{S3_IMAGE}food-101.tgz'\n",
    "    MNIST        = f'{S3_IMAGE}mnist_png.tgz'\n",
    "    PETS         = f'{S3_IMAGE}oxford-iiit-pet.tgz'\n",
    "\n",
    "    # NLP datasets\n",
    "    AG_NEWS                 = f'{S3_NLP}ag_news_csv.tgz'\n",
    "    AMAZON_REVIEWS          = f'{S3_NLP}amazon_review_full_csv.tgz'\n",
    "    AMAZON_REVIEWS_POLARITY = f'{S3_NLP}amazon_review_polarity_csv.tgz'\n",
    "    DBPEDIA                 = f'{S3_NLP}dbpedia_csv.tgz'\n",
    "    MT_ENG_FRA              = f'{S3_NLP}giga-fren.tgz'\n",
    "    SOGOU_NEWS              = f'{S3_NLP}sogou_news_csv.tgz'\n",
    "    WIKITEXT                = f'{S3_NLP}wikitext-103.tgz'\n",
    "    WIKITEXT_TINY           = f'{S3_NLP}wikitext-2.tgz'\n",
    "    YAHOO_ANSWERS           = f'{S3_NLP}yahoo_answers_csv.tgz'\n",
    "    YELP_REVIEWS            = f'{S3_NLP}yelp_review_full_csv.tgz'\n",
    "    YELP_REVIEWS_POLARITY   = f'{S3_NLP}yelp_review_polarity_csv.tgz'\n",
    "\n",
    "    # Image localization datasets\n",
    "    BIWI_HEAD_POSE     = f\"{S3_IMAGELOC}biwi_head_pose.tgz\"\n",
    "    CAMVID             = f'{S3_IMAGELOC}camvid.tgz'\n",
    "    CAMVID_TINY        = f'{URL}camvid_tiny.tgz'\n",
    "    LSUN_BEDROOMS      = f'{S3_IMAGE}bedroom.tgz'\n",
    "    PASCAL_2007        = f'{S3_IMAGELOC}pascal_2007.tgz'\n",
    "    PASCAL_2012        = f'{S3_IMAGELOC}pascal_2012.tgz'\n",
    "\n",
    "    # Audio classification datasets\n",
    "    MACAQUES           = 'https://storage.googleapis.com/ml-animal-sounds-datasets/macaques.zip'\n",
    "    ZEBRA_FINCH        = 'https://storage.googleapis.com/ml-animal-sounds-datasets/zebra_finch.zip'\n",
    "\n",
    "    # Medical Imaging datasets\n",
    "    #SKIN_LESION        = f'{S3_IMAGELOC}skin_lesion.tgz'\n",
    "    SIIM_SMALL         = f'{S3_IMAGELOC}siim_small.tgz'\n",
    "    TCGA_SMALL         = f'{S3_IMAGELOC}tcga_small.tgz'\n",
    "\n",
    "    #Pretrained models\n",
    "    OPENAI_TRANSFORMER = f'{S3_MODEL}transformer.tgz'\n",
    "    WT103_FWD          = f'{S3_MODEL}wt103-fwd.tgz'\n",
    "    WT103_BWD          = f'{S3_MODEL}wt103-bwd.tgz'\n",
    "\n",
    "    def path(url='.', c_key='archive'):\n",
    "        \"Return local path where to download based on `c_key`\"\n",
    "        fname = url.split('/')[-1]\n",
    "        local_path = URLs.LOCAL_PATH/('models' if c_key=='models' else 'data')/fname\n",
    "        if local_path.exists(): return local_path\n",
    "        return Config()[c_key]/fname\n",
    "\n",
    "# Cell\n",
    "def download_url(url, dest, overwrite=False, pbar=None, show_progress=True, chunk_size=1024*1024,\n",
    "                 timeout=4, retries=5):\n",
    "    \"Download `url` to `dest` unless it exists and not `overwrite`\"\n",
    "    if os.path.exists(dest) and not overwrite: return\n",
    "\n",
    "    s = requests.Session()\n",
    "    s.mount('http://',requests.adapters.HTTPAdapter(max_retries=retries))\n",
    "    # additional line to identify as a firefox browser, see fastai/#2438\n",
    "    s.headers.update({'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:71.0) Gecko/20100101 Firefox/71.0'})\n",
    "    u = s.get(url, stream=True, timeout=timeout)\n",
    "    try: file_size = int(u.headers[\"Content-Length\"])\n",
    "    except: show_progress = False\n",
    "\n",
    "    with open(dest, 'wb') as f:\n",
    "        nbytes = 0\n",
    "        if show_progress: pbar = progress_bar(range(file_size), leave=False, parent=pbar)\n",
    "        try:\n",
    "            if show_progress: pbar.update(0)\n",
    "            for chunk in u.iter_content(chunk_size=chunk_size):\n",
    "                nbytes += len(chunk)\n",
    "                if show_progress: pbar.update(nbytes)\n",
    "                f.write(chunk)\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            fname = url.split('/')[-1]\n",
    "            data_dir = dest.parent\n",
    "            print(f'\\n Download of {url} has failed after {retries} retries\\n'\n",
    "                  f' Fix the download manually:\\n'\n",
    "                  f'$ mkdir -p {data_dir}\\n'\n",
    "                  f'$ cd {data_dir}\\n'\n",
    "                  f'$ wget -c {url}\\n'\n",
    "                  f'$ tar xf {fname}\\n'\n",
    "                  f' And re-run your code once the download is successful\\n')\n",
    "\n",
    "# Cell\n",
    "def download_data(url, fname=None, c_key='archive', force_download=False, timeout=4):\n",
    "    \"Download `url` to `fname`.\"\n",
    "    fname = Path(fname or URLs.path(url, c_key=c_key))\n",
    "    fname.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not fname.exists() or force_download: download_url(url, fname, overwrite=force_download, timeout=timeout)\n",
    "    return fname\n",
    "\n",
    "# Cell\n",
    "def _get_check(url):\n",
    "    \"internal function to get the hash of the file at `url`.\"\n",
    "    checks = json.load(open(Path(__file__).parent/'checks.txt', 'r'))\n",
    "    return checks.get(url, '')\n",
    "\n",
    "def _check_file(fname):\n",
    "    \"internal function to get the hash of the local file at `fname`.\"\n",
    "    size = os.path.getsize(fname)\n",
    "    with open(fname, \"rb\") as f: hash_nb = hashlib.md5(f.read(2**20)).hexdigest()\n",
    "    return [size,hash_nb]\n",
    "\n",
    "# Cell\n",
    "def _add_check(url, fname):\n",
    "    \"Internal function to update the internal check file with `url` and check on `fname`.\"\n",
    "    checks = json.load(open(Path(__file__).parent/'checks.txt', 'r'))\n",
    "    checks[url] = _check_file(fname)\n",
    "    json.dump(checks, open(Path(__file__).parent/'checks.txt', 'w'), indent=2)\n",
    "\n",
    "# Cell\n",
    "def file_extract(fname, dest=None):\n",
    "    \"Extract `fname` to `dest` using `tarfile` or `zipfile`.\"\n",
    "    if dest is None: dest = Path(fname).parent\n",
    "    fname = str(fname)\n",
    "    if   fname.endswith('gz'):  tarfile.open(fname, 'r:gz').extractall(dest)\n",
    "    elif fname.endswith('zip'): zipfile.ZipFile(fname     ).extractall(dest)\n",
    "    else: raise Exception(f'Unrecognized archive: {fname}')\n",
    "\n",
    "# Cell\n",
    "def _try_from_storage(dest, storage):\n",
    "    \"an internal function to create symbolic links for files from `storage` to `dest` if `storage` exists\"\n",
    "    if not storage.exists(): return\n",
    "    os.makedirs(dest, exist_ok=True)\n",
    "    for f in storage.glob('*'): os.symlink(f, dest/f.name, target_is_directory=f.is_dir())\n",
    "\n",
    "# Cell\n",
    "def newest_folder(path):\n",
    "    \"Return newest folder on path\"\n",
    "    list_of_paths = path.glob('*')\n",
    "    return max(list_of_paths, key=lambda p: p.stat().st_ctime)\n",
    "\n",
    "# Cell\n",
    "def rename_extracted(dest):\n",
    "    \"Rename file if different from dest\"\n",
    "    extracted = newest_folder(dest.parent)\n",
    "    if not (extracted.name == dest.name): extracted.rename(dest)\n",
    "\n",
    "# Cell\n",
    "def untar_data(url, fname=None, dest=None, c_key='data', force_download=False, extract_func=file_extract, timeout=4):\n",
    "    \"Download `url` to `fname` if `dest` doesn't exist, and un-tgz or unzip to folder `dest`.\"\n",
    "    default_dest = URLs.path(url, c_key=c_key).with_suffix('')\n",
    "    dest = default_dest if dest is None else Path(dest)/default_dest.name\n",
    "    fname = Path(fname or URLs.path(url))\n",
    "    if fname.exists() and _get_check(url) and _check_file(fname) != _get_check(url):\n",
    "        print(\"A new version of this dataset is available, downloading...\")\n",
    "        force_download = True\n",
    "    if force_download:\n",
    "        if fname.exists(): os.remove(fname)\n",
    "        if dest.exists(): shutil.rmtree(dest)\n",
    "    if not dest.exists(): _try_from_storage(dest, URLs.path(url, c_key='storage').with_suffix(''))\n",
    "    if not dest.exists():\n",
    "        fname = download_data(url, fname=fname, c_key=c_key, timeout=timeout)\n",
    "        if _get_check(url) and _check_file(fname) != _get_check(url):\n",
    "            print(f\"File downloaded is broken. Remove {fname} and try again.\")\n",
    "        extract_func(fname, dest.parent)\n",
    "        rename_extracted(dest)\n",
    "    return dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6b007471336f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m \u001b[0;31m# if you haven't already done so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path # if you haven't already done so\n",
    "file = Path(__file__).resolve()\n",
    "parent, root = file.parent, file.parents[1]\n",
    "sys.path.append(str(root))\n",
    "\n",
    "# Additionally remove the current file's directory from sys.path\n",
    "try:\n",
    "    sys.path.remove(str(parent))\n",
    "except ValueError: # Already removed\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mypackage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-fd2f9c6e9496>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSCRIPT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPACKAGE_PARENT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmypackage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmymodule\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mas_int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mypackage'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "PACKAGE_PARENT = '..'\n",
    "SCRIPT_DIR = os.path.dirname(os.path.realpath(os.path.join(os.getcwd(), os.path.expanduser(\"__file__\"))))\n",
    "sys.path.append(os.path.normpath(os.path.join(SCRIPT_DIR, PACKAGE_PARENT)))\n",
    "\n",
    "from mypackage.mymodule import as_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'URLs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-de52cc2057d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muntar_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURLs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNIST_SAMPLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'URLs' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "https://github.com/fastai/fastai/blob/master/fastai/data/external.py#L244\n",
    "'''\n",
    "def file_extract(fname, dest=None):\n",
    "    \"Extract `fname` to `dest` using `tarfile` or `zipfile`.\"\n",
    "    if dest is None: dest = Path(fname).parent\n",
    "    fname = str(fname)\n",
    "    if   fname.endswith('gz'):  tarfile.open(fname, 'r:gz').extractall(dest)\n",
    "    elif fname.endswith('zip'): zipfile.ZipFile(fname     ).extractall(dest)\n",
    "    else: raise Exception(f'Unrecognized archive: {fname}')\n",
    "\n",
    "\n",
    "def untar_data(url, fname=None, dest=None, c_key='data', force_download=False, extract_func=file_extract, timeout=4):\n",
    "    \"Download `url` to `fname` if `dest` doesn't exist, and un-tgz or unzip to folder `dest`.\"\n",
    "    default_dest = URLs.path(url, c_key=c_key).with_suffix('')\n",
    "    dest = default_dest if dest is None else Path(dest)/default_dest.name\n",
    "    fname = Path(fname or URLs.path(url))\n",
    "    if fname.exists() and _get_check(url) and _check_file(fname) != _get_check(url):\n",
    "        print(\"A new version of this dataset is available, downloading...\")\n",
    "        force_download = True\n",
    "    if force_download:\n",
    "        if fname.exists(): os.remove(fname)\n",
    "        if dest.exists(): shutil.rmtree(dest)\n",
    "    if not dest.exists(): _try_from_storage(dest, URLs.path(url, c_key='storage').with_suffix(''))\n",
    "    if not dest.exists():\n",
    "        fname = download_data(url, fname=fname, c_key=c_key, timeout=timeout)\n",
    "        if _get_check(url) and _check_file(fname) != _get_check(url):\n",
    "            print(f\"File downloaded is broken. Remove {fname} and try again.\")\n",
    "        extract_func(fname, dest.parent)\n",
    "        rename_extracted(dest)\n",
    "    return dest\n",
    "\n",
    "\n",
    "path = untar_data(URLs.MNIST_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(os.path.dirname(\"test.py\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
